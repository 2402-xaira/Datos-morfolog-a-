# -*- coding: utf-8 -*-
"""01 Morfotipos sin filogenia.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16lJ54sUs9CrHpQOTNn0PEQHsYQ8888t4

# **Definir grupos morfológicos sin corrección filogenética**
<img src="https://scontent.fbog5-1.fna.fbcdn.net/v/t1.18169-9/602235_270051553129701_924082703_n.jpg?_nc_cat=101&ccb=1-7&_nc_sid=cdbe9c&_nc_eui2=AeE9maJh-MPAdKMqhc1ACllW-Z55pUKy6-z5nnmlQrLr7L2d5uVqjVNTIyyRU5ODQsISujtUTlFVYQ0A9ehDLggj&_nc_ohc=4uXZNRXU9FcAX967X4I&_nc_ht=scontent.fbog5-1.fna&oh=00_AfDsQRu-fAYPS6Bj4YWG5CCnaYbCl1BtgDB5Z86d68PpxQ&oe=65418C55" width=350, align ="center">

La delimitación de morfotipos es el primer paso en estudios de ecomorfología. Generalmente, son características de morfología externa las que se requieren para definir grupos morfológicos. Una vez delimitados estos grupos, el siguiente paso es evaluar la relación morfología-uso de recursos.


Un ejemplo clásico de morfotipos es el de los pinzones de las Islas Galápagos, estudiados por Charles Darwin. Estos pinzones presentan diferentes tipos de picos que reflejan su adaptación a los diferentes nichos alimenticios disponibles en cada isla. Algunos pinzones tienen picos adaptados para romper semillas duras, mientras que otros tienen picos más largos y finos para extraer insectos de la corteza de los árboles. Cada uno de estos morfotipos representa una respuesta evolutiva a la disponibilidad de alimentos en su entorno.

Delimitaremos grupos morfológicos con datos de morfometría y conteo de lamelas en especies del género Anolis. Este género de lagartijas es también un ejemplo de cómo la forma del cuerpo y número de lamelas permite reunir en grupos morfológicos a varias especies que comparten el uso del microhábitat estructural. La similitud en morfología, comportamiento y uso del microhábitat permite la delimitación posterior de ecomorfos.

En esta primera parte la delimitación de los grupos morfológicos o morfotipos se hará con métodos multivariados sin tener en cuenta la filogenia.
"""

install.packages(c("factoextra", "GGally", "cluster", "tidyverse"))

library(factoextra)
library(GGally)
library(cluster)
library(tidyverse)

"""# **Descripción de los datos**
- **anole.data.csv:** Rasgos continuos en mm, **100** especies de *Anolis* del Caribe en formato CSV. Fuente: Mahler et al. (2010). Los datos de la publicación mencionada están ya transformados, pero trabajaremos con la tabla de datos crudos en mm y número promedio de lamelas, sin transformar.

    - **SVL**: tamaño corporal
    - **HL**: longitud de la cabeza
    - **FLL**: longitud de la pata delantera
    - **HLL**: longitud de la pata trasera
    - **TL**: longitud de la cola
    - **LAM**: número de lamelas subdigitales en las falanges 2 y 3 del cuarto dedo de la pata posterior.

- **Anolis.tre:** Arbol filogenético calibrado para 100 especies de Anolis del Caribe en formato Newick. Fuente: Mahler et al (2010).
"""

anole.data <- read.csv("anole.data.csv", row.names=1)
head(anole.data)

"""# **Correción alómetrica**
El primer paso es transformar los datos con logaritmo. El segundo es corregir el posible efecto del tamaño corporal sobre los rasgos, en especial, los rasgos morfométricos.

Para la corrección se hace a través de una regresión lineal de los rasgos o variables vs. el tamaño corporal. De esta manera se obtienen los residuos que serán nuestras nuevas variables para los análisis siguientes.
"""
ggpairs(anole.data)

# Nombres de las columnas
names = c("HL", "HLL", "FLL", "LAM", "TL")  # Asegúrate de que estos nombres coinciden con los nombres de las columnas de ecomorph.data

# Calcula el logaritmo de SVL una vez, fuera del bucle
SVL = log(anole.data[,"SVL"])
names(SVL) = row.names(anole.data)

# Bucle para aplicar log, calcular residuos y asignar a variables
for (i in 1:length(names)) {
  # Calcula el logaritmo de la columna actual
  log_vector = log(anole.data[,names[i]])
  names(log_vector) = row.names(anole.data)

  # Calcula residuos
  fit.ols = lm(log(log_vector)~log(SVL), anole.data)

  # Asigna los residuos a la variable correspondiente en el entorno global
  assign(names[i], fit.ols$residual)
}

species = row.names(anole.data)
data_resid = data.frame(cbind(SVL, HL, HLL, FLL, LAM, TL), row.names  = species)
head(data_resid)

"""- **Trabajaremos en adelante con los residuales de la regresión.**

Como se observa, ya ninguna de las variables morfométricas ni el conteo de lamelas está relacionada con el tamaño corporal.
"""

ggpairs(data_resid)


##Otra forma de visualizar la relación entre las variables y su contribución
#a la variación total: PCA o análisis de componentes principales


"""# **Ahora exploraremos el comportamiento de cada rasgo sobre la variación de los datos y la relación entre los rasgos analizados con un PCA**
## **PCA**
El Análisis de Componentes Principales (PCA, por sus siglas en inglés) es una técnica de reducción de dimensionalidad utilizada en estadísticas y aprendizaje automático. Su objetivo principal es transformar un conjunto de datos con muchas variables en un nuevo conjunto de variables llamadas "componentes principales" que son combinaciones lineales de las variables originales. Estas combinaciones se eligen de manera que capturen la mayor variabilidad posible en los datos.
"""

res.pca <- prcomp(data_resid, scale = TRUE)
eig.val <- get_eigenvalue(res.pca)
summary(res.pca)

"""Vemos que son necesarios los tres primeros componentes para explicar el 71% de la variación en los datos."""

fviz_eig(res.pca)

res.pca$rotation
head(res.pca$x, 4)

"""En estas tablas visualizamos la "carga" de cada rasgo o variable sobre cada componente. En su órden en los tres primeros componentes las variables que explican el patrón de la variación en los datos son: HLL, HL y SVL, los valores de estas variables sobre los componentes son los que más se alejan de 0.

En la segunda tabla se muestran los "scores" que son las coordenadas que permiten ubicar cada especie en el espacio morfológico definido por cada componente.
"""

fviz_pca_var(res.pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
)

res.pca$x

###### otros scripts PCA y gráfico de datos de la especie en el morfoespacio del PCA
library(ggplot2)
library(dplyr)
library(ggfortify)
library(stats)



# Convertir a data frame
pca_df <- as.data.frame(res.pca$x, rownames=species)

head(pca_df)

pca_df$Species <- as.factor(species)  # Asignar nombres de especies

# Obtener loadings (variables originales)
loadings <- as.data.frame(res.pca$rotation[, 1:2])  # Componentes principales 1 y 2
loadings$Variable <- rownames(loadings)  # Agregar nombres de variables

# Escalar los loadings para visualización
scale_factor <- max(abs(pca_df$PC1), abs(pca_df$PC2))  # Ajustar escala de flechas
loadings <- loadings %>% mutate(PC1 = PC1 * scale_factor, PC2 = PC2 * scale_factor)


# Graficar
ggplot() +
  # Puntos de los individuos
  geom_point(data = pca_df, aes(x = PC1, y = PC2, color = Species), alpha = 0.7) +
  # Líneas de referencia en el origen
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  geom_vline(xintercept = 0, linetype = "dashed", color = "black") +
  # Flechas de las variables originales
  geom_segment(data = loadings, aes(x = 0, y = 0, xend = PC1, yend = PC2),
               arrow = arrow(length = unit(0.2, "cm")), color = "blue", size = 0.8) +
  # Etiquetas de las variables originales
  geom_text(data = loadings, aes(x = PC1, y = PC2, label = Variable), 
            vjust = 1.2, hjust = 1.2, color = "blue") +
  # Etiquetas y tema
  labs(title = 'PCA: Componentes Principales con Variables Originales',
       x = paste0('PC1 (', round(summary(res.pca)$importance[2,1] * 100, 2), '% varianza)'),
       y = paste0('PC2 (', round(summary(res.pca)$importance[2,2] * 100, 2), '% varianza)')) +
  theme_minimal() +
  theme(legend.position = 'none',
        panel.border = element_rect(color = 'black', fill = NA, size = 1),
        axis.line = element_line(color = 'black'))




#Extraer las variables que presentan colinealidad (HLL y FLL), elegir una de ellas

data_resid$HLL=NULL
head(data_resid)

"""# **Delimitación de los grupos morfológicos (Morfotipos)**
Para definir cuántos grupos morfológicos se pueden delimitar a partir de los datos, utilizaremos algoritmos de agrupación o clusterización. Para este ejemplo práctico presentamos 3 algoritmos diferentes:

- **K-means:** Es un algoritmo de aprendizaje no supervisado utilizado para agrupar datos en k grupos distintos. Funciona asignando puntos de datos a un centroide cercano y recalculando los centroides hasta que se minimiza la distancia entre los puntos y sus centroides asignados. Es adecuado para datos numéricos y encuentra grupos compactos y bien separados. Es un método aglomerativo.

- **K-medians:** Es similar al algoritmo K-means, pero en lugar de calcular los centroides como la media aritmética, se utiliza la mediana. Esto lo hace más robusto ante valores extremos (outliers) en los datos, ya que no se ven afectados tanto por valores atípicos. Es un método aglomerativo.

- **Jerárquico divisivo:** Es un método de agrupamiento jerárquico que comienza con un solo grupo que contiene todos los datos y luego divide repetidamente los grupos en subgrupos más pequeños. En cada paso, se selecciona el grupo más similar y se divide en dos hasta que cada punto esté en su propio grupo. Esto crea una jerarquía de grupos anidados, que puede visualizarse en un dendrograma.

<hr/>
<div class="alert alert-success alertsuccess" style="margin-top: 20px">
[Nota]:
Mientras que K-Means utiliza la distancia euclidiana y K-Medians utiliza la distancia de Manhattan, el agrupamiento jerárquico divisivo puede utilizar una variedad de métricas de distancia en función de la elección del usuario y la naturaleza de los datos.
</div>
<hr/>


# **Métodos para evaluar la cantidad óptima de  grupos sugeridos por los métodos aglomerativos y divisivos**

Estos métodos son aplicados a cada algoritmo empleado en el paso anterior.

1. **Método del Codo (Elbow Method):**
   
   El método del codo es una técnica visual que ayuda a determinar el número óptimo de clústeres en un conjunto de datos utilizando el algoritmo de K-Means. La idea es calcular la variabilidad intra-cluster (dentro del clúster) para diferentes valores de k (número de clústeres) y luego observar la curva resultante. En general, a medida que aumentamos k, la variabilidad intra-cluster disminuirá, ya que cada punto estará más cerca de su centroide correspondiente. Sin embargo, llega un punto en el que agregar más clústeres no proporciona una mejora significativa en la reducción de la variabilidad intra-cluster. Esto a menudo se refleja en la gráfica como un "codo", donde el cambio en la reducción de la variabilidad se desacelera notablemente. El número de clústeres en el punto del "codo" se considera una buena estimación del número óptimo de clústeres.

2. **Método Silhouette:**
   
   El método Silhouette es otra técnica para evaluar la calidad de los clústeres formados en un análisis de clustering. Calcula un valor de silueta para cada punto en función de su similitud con su propio clúster en comparación con otros clústeres cercanos. El valor de silueta varía entre -1 y 1, donde:
   
   - Un valor cercano a +1 indica que el punto está bien clasificado en su propio clúster y se encuentra a una buena distancia de los clústeres vecinos.
   - Un valor cercano a 0 indica que el punto está cerca del límite entre dos clústeres.
   - Un valor cercano a -1 indica que el punto está mal clasificado en su propio clúster y sería mejor si estuviera en un clúster vecino.
   
   En general, un valor de silueta promedio más alto sugiere una mejor separación entre los clústeres y, por lo tanto, una asignación de clúster más adecuada.

3. **Método Gap Statistic:**
   
   La estadística Gap es una medida que compara la distribución de los datos dentro de los clústeres con la distribución que se esperaría si los datos se distribuyeran al azar. Para evaluar el número óptimo de clústeres, se compara la estadística Gap para diferentes valores de k con un conjunto de datos aleatorios. Si la estadística Gap es significativamente más grande para un valor de k que para otros, esto sugiere que ese valor de k es más adecuado para describir la estructura de los datos que valores más altos o más bajos.

4. **Estadístico de pseudo T2
Es una medida de varianza basada en el agrupamiento jerárquico.
Altos valores indican una separación fuerte entre clusters, y una caída aguda señala un número óptimo de clusters.

#commonly used with Ward's method (implementado en el método jerárquico divisivo) to stop merging at the right point.


<hr/>
<div class="alert alert-success alertsuccess" style="margin-top: 20px">
[Resumen]:
Estos métodos son enfoques utilizados para tomar decisiones informadas sobre el número de clústeres adecuado en un análisis de clustering. El método del codo y la estadística Gap se basan en medidas de variabilidad, mientras que el método Silhouette se enfoca en la cohesión y separación de los puntos dentro de los clústeres.
</div>
<hr/>

# **K-means**
"""

options(repr.plot.width = 5, repr.plot.height = 5)

# Elbow method
fviz_nbclust(anole.data, kmeans, method = "wss") +
    geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")

##Para crear el objeto del resultado de silhouette se asigna un nombre
kmeans_result=fviz_nbclust(anole.data, kmeans, method = "wss") +
  geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")


# Silhouette method
fviz_nbclust(data_resid, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")

##Para crear el objeto del resultado de silhouette se asigna un nombre
SHresult=fviz_nbclust(data_resid, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")

# Gap statistic
# nboot = 50 to keep the function speedy.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
set.seed(123)
fviz_nbclust(data_resid, kmeans, nstart = 25,  method = "gap_stat", nboot = 50)+
  labs(subtitle = "Gap statistic method")



##Extraer la identidad de los datos asignados a los clusters por k con el método Elbow
library (ggplot2)
library(dplyr)

# 2. Aplicar K-Means con 4 clusters
kmeans_result <- kmeans(data_resid, centers = 4)


# 5. Visualizar los clusters usando los scores de los dos primeros pca

## para esto primero correr el pca, extraer los PC1 y PC2 y asignar la columna de los cluster

data_resid$cluster=NULL #para eliminar la columna asignada en el paso previo para correr el pca

print(data_resid)


res.pca <- prcomp(data_resid, scale = TRUE)
eig.val <- get_eigenvalue(res.pca)
summary(res.pca)


##Extraer los scores de los componentes del pca
data_scores=res.pca$x

##Crear el objeto de lista de especies
species=rownames(data_resid)


##Guardar como dataframe los scores del pca
data_scoresDF=data.frame(data_scores)

#
# 3. Agregar la asignación de clusters a los datos. Se adiciona esta columna al dataframe que se desee graficar


data_scoresDF$cluster <- as.factor(kmeans_result$cluster)

## Graficar PC1 y PC2 identificando por clusters

library(ggplot2)
ggplot(data_scoresDF, aes(PC1, PC2, color = data_scoresDF$cluster)) +
  geom_point(size = 3) +
  theme_minimal() +
  labs(title = "Clustering con K-Means", x = "PC1", y = "PC2")

############################################################################

"""# **K-Medians**"""

options(repr.plot.width = 5, repr.plot.height = 5)
# Elbow method
fviz_nbclust(data_resid, pam, method = "wss") +
    geom_vline(xintercept = 3, linetype = 2)+
  labs(subtitle = "Elbow method")

# Silhouette method
fviz_nbclust(data_resid, pam, method = "silhouette")+
  labs(subtitle = "Silhouette method")

# Gap statistic
# nboot = 50 to keep the function speedy.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
set.seed(123)
fviz_nbclust(data_resid, pam, nstart = 25,  method = "gap_stat", nboot = 50)+
  labs(subtitle = "Gap statistic method")

"""# **Jerárquico divisivo**"""


# Compute distance matrix
dist_matrix <- dist(data_resid, method = "manhattan")

# Perform hierarchical clustering using Ward's method
hc <- hclust(dist_matrix, method = "ward.D2")

# Extract merge heights from hclust object
merge_heights <- rev(hc$height)  # Reverse order to match cluster steps

# Plot dendrogram
plot(hc, main = "Hierarchical Clustering Dendrogram", xlab = "", sub = "")

# Elbow method
fviz_nbclust(data_resid, hcut, method = "wss") +
    geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")

optimal_k <- 4

# Cortar el dendrograma para obtener las etiquetas de los clusters
cluster_labels <- cutree(hc, k = optimal_k)

# Visualizar el dendrograma con clusters resaltados
fviz_dend(hc, k = optimal_k, 
          rect = TRUE,          # Resalta los clusters con rectángulos
          rect_border = "red",  # Color de los bordes de los rectángulos
          rect_fill = TRUE,     # Rellena los rectángulos
          main = "Dendrograma con Clusters"
)

# Silhouette method
fviz_nbclust(data_resid, hcut, method = "silhouette")+
  labs(subtitle = "Silhouette method")

optimal_k <- 2

# Cortar el dendrograma para obtener las etiquetas de los clusters
cluster_labels <- cutree(hc, k = optimal_k)

# Visualizar el dendrograma con clusters resaltados
fviz_dend(hc, k = optimal_k, 
          rect = TRUE,          # Resalta los clusters con rectángulos
          rect_border = "red",  # Color de los bordes de los rectángulos
          rect_fill = TRUE,     # Rellena los rectángulos
          main = "Dendrograma con Clusters"
)


# Gap statistic
# nboot = 50 to keep the function speedy.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
set.seed(123)
fviz_nbclust(data_resid, hcut, nstart = 25,  method = "gap_stat", nboot = 50)+
  labs(subtitle = "Gap statistic method")

optimal_k <- 3

# Cortar el dendrograma para obtener las etiquetas de los clusters
cluster_labels <- cutree(hc, k = optimal_k)

# Visualizar el dendrograma con clusters resaltados
fviz_dend(hc, k = optimal_k, 
          rect = TRUE,          # Resalta los clusters con rectángulos
          rect_border = "red",  # Color de los bordes de los rectángulos
          rect_fill = TRUE,     # Rellena los rectángulos
          main = "Dendrograma con Clusters"
)


# Function to compute pseudo T^2 values
compute_pseudo_T2 <- function(hc) {
  n <- length(hc$height)  # Number of merges
  pseudo_T2 <- numeric(n) # Vector to store pseudo T2 values
  
  for (i in 1:(n-1)) {
    SS_After <- hc$height[i]     # Between-cluster sum of squares after merging
    SS_Before <- hc$height[i + 1]  # Before the next merge
    
    # Compute pseudo T^2
    pseudo_T2[i] <- (SS_Before - SS_After) / SS_After
  }
  
  # The last value is undefined (set as NA)
  pseudo_T2[n] <- NA
  
  return(pseudo_T2)
}

# Compute pseudo T2 values
pseudoT2_values <- compute_pseudo_T2(hc)

# Compute number of clusters at each step
num_clusters <- rev(2:(length(hc$height) + 1))  # From max clusters to 2

# Create a data frame with clustering steps, merged clusters, heights, and pseudo T^2
pseudoT2_table <- data.frame(
  Step = 1:length(pseudoT2_values),
  Num_Clusters = num_clusters,  # Include number of clusters at each step
  Merged_Clusters = paste(hc$merge[,1], hc$merge[,2], sep = " & "),  # Show merged clusters
  Merge_Height = hc$height,  # Heights at which merging occurs
  PseudoT2 = pseudoT2_values  # Computed pseudo T^2 values
)

# Print first few rows
head(pseudoT2_table)
pseudoT2_table


# Visualizar Pseudo T²
plot(pseudoT2_values, type = "b", pch = 19, col = "blue",
     main = "Pseudo T² para el Análisis Jerárquico", 
     xlab = "Paso de Fusión", ylab = "Pseudo T²")

##En este ejemplo, en el cluster jerárquico, según pseudoT2 el número
#de clusters elegido es 4, en el paso 97 se observa una caída dramática en el
#estadístico

"""De acuerdo a los resultados de los tres algoritmos (K-means, K-medians y divisivo jerárquico), para cada uno de los métodos de evaluación del número óptimo de clusters, los agrupamientos óptimos oscilan entre 1 a 8 clusters. Entonces, ¿cuál es el número óptimo de clusters en los que se pueden dividir nuestras especies? Para responder esta pregunta sería necesario probar cada hipótesis del número de clusters.


En este caso probaremos la hipótesis de 4 clusters con un análisis de función discriminante.
Para esto, es necesario identificar qué especies pertenecen a cada uno de los clusters.

**Primero con el método jerárquico divisivo**
"""

hcut.res <- hcut(data_resid, 4)
print(hcut.res)

hcut.res <- data.frame(cluster = hcut.res$cluster, row.names = row.names(data_resid))
dim(hcut.res)
hcut.res %>%
    count(cluster)




"""# **Grupos morfológicos**
Podemos visualizar las especies que pertenecen a cada grupo morfológico y guardaremos este nuevo dataframe.
"""

grupos_morfologicos <- data_resid
grupos_morfologicos$cluster <- hcut.res$cluster

## Guardamos nuestro nuevo dataframe que en este caso quedara guardado en la carpeta Datos Anolis
write.csv(grupos_morfologicos, file = "grupos_morfologicos.csv")

# filtrar por cluster
cluster1 <- grupos_morfologicos %>% filter(cluster == 1)
cluster2 <- grupos_morfologicos %>% filter(cluster == 2)
cluster3 <- grupos_morfologicos %>% filter(cluster == 3)
cluster4 <- grupos_morfologicos %>% filter(cluster == 4)

cluster1

cluster2

cluster3

cluster4

####DFA
library(MASS)
library(klaR)
library(pander)

dfa.anole <- data.frame(grupos_morfologicos)
dfa.anole$cluster <- factor(hcut.res$cluster)

dfa <- lda(dfa.anole$cluster ~ SVL + HL + HLL + FLL + LAM + TL, 
           data = dfa.anole, na.action = "na.omit", CV = FALSE)

dfa$counts # número de especies por grupo
dfa$scaling # contribución de cada variable por función


# 3. Ver los coeficientes de las funciones discriminantes
print(dfa$scaling)

# Extraer los scores de cada observación en las funciones discriminantes
scores <- predict(dfa)$x  

# Ver los primeros valores de los scores
head(scores)


p1 = predict(dfa, dfa.anole)$class
(tab = table(Predicted = p1, Actual = dfa.anole$cluster)) # muestra la matriz de prediccion
sum(diag(tab))/sum(tab) #matriz de exactitud

p <- predict(dfa)
freqtable <- table(p$class, dfa.anole$cluster)

data.frame(p$posterior, ObservedGroup = dfa.anole$cluster,
           PredictedGroup = p$class, p$x) %>%
  head(48) %>%
  pander

#Guardar la tabla de frecuencias resultado del DFA
write.table(freqtable, file = "tabla_frecuencias.txt", sep = "\t", col.names = NA, quote = FALSE)

# Realizar predicciones sobre los mismos datos (puedes usar nuevos datos)
predictions <- predict(dfa, dfa.anole)

# Extraer los grupos observados y predicciones
class_predictions <- data.frame(Rownames = rownames(anole.data),ObservedGroup = dfa.anole$cluster,Predicted = predictions$class)

# Ver las primeras filas de las predicciones
head(class_predictions)

class_predictions


# 4. Significancia de las funciones con Wilks' Lambda
library(MASS)  # Para pruebas de significancia

# Realizar una prueba MANOVA para ver la significancia de las funciones discriminantes
#Si es significativa quiere decir que las funciones separan los grupos evaluados

manova_model <- manova(cbind(SVL, HL, FLL, LAM, TL) ~ dfa.anole$cluster, data = dfa.anole)
summary(manova_model, test = "Wilks")


# Realizar un ANOVA para cada función discriminante
anova_results <- apply(scores, 2, function(x) summary(aov(x ~ dfa.anole$cluster)))

# Ver los resultados del ANOVA
anova_results



#Ahora graficar

data_scoresDF$LDA1 <- p$x[,1]
data_scoresDF$LDA2 <- p$x[,2]


# hacemos un grafico de densidad para cada funcion
# Calcular las medias por grupo

ggplot(data_scoresDF, aes(LDA1, fill= cluster)) +
  geom_density(alpha = 0.5, color = NA) +
  theme(legend.position = "top")

#Visualizar
gMeans <- data_scoresDF %>%
  group_by(cluster) %>%
  summarize(LDA1 = mean(LDA1), LDA2 = mean(LDA2))

gMeans %>%
  pander

options(repr.plot.width = 10, repr.plot.height = 5)

ggplot(data_scoresDF, aes(LDA1, LDA2, color = cluster)) +
  geom_point(alpha = 0.5,
             size = 5) +
  geom_text(data = gMeans,
            aes(label = cluster),
            color = "green",
            vjust = 1.75) +
  geom_point(data = gMeans,
             aes(fill = cluster),
             size = 7,
             color = "pink",
             pch = 21) +
  theme(legend.position = "down") +
  coord_equal() + theme_bw() + ggtitle("Morfotipos") +
  labs(x = "DF1 = 70.66%", y = "DF2 = 24.54%") +
  theme(plot.title = element_text(hjust = 0.5))



"""#FIN MÉTODOS DE AGRUPAMIENTO"""###



